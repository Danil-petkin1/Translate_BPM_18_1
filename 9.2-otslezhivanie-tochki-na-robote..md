# 9.2 Отслеживание точки на роботе.

Робот, работающий в системе ROS, всегда может определить 3D-положение и ориентацию каждого из своих звеньев и шарниров. Это становится возможным благодаря модели URDF и узлу `robot_state_publisher`, которые вместе поддерживают синхронизацию дерева `tf` с текущими совместными состояниями робота. В результате, мы можем использовать наш узел отслеживания головы для отслеживания любой части робота по мере его движения. Например, если робот передает кому-то объект, мы можем запрограммировать голову на отслеживание местоположения захвата по мере того, как рука протягивает руку наружу к человеку. Получается, что такое движение головы при передаче объекта кому-то другому, является важным социальным человеческим сигналом , который сигнализирует о том, что приемник должен протянуть руку, чтобы забрать объект. 

Положение точки на роботе можно задать относительно любой удобной системы координат. Для отслеживания конкретной ссылки в URDF-модели робота можно просто использовать координаты \(0, 0, 0\) в системе отсчета этой ссылки. Таким образом, чтобы отследить местоположение захвата, голова должна следовать за точкой \(0, 0, 0\) в рамке захвата.

Скрипт, называемый `pub_tf_frame.py` в каталоге `rbx2_utils/nodes` просто публикует сообщение с `PoseStamped` на тему target\_pose с координатами \(0, 0, 0\) и ориентацией \(0, 0, 0, 1\) относительно рамки отсчета, которую мы хотим отследить. По умолчанию целевым фреймом в скрипте является `right_gripper_link`.

Чтобы попробовать, используйте все те же стартовые файлы, что и в предыдущем разделе, но `Ctrl-C` из файла `pub3d_target.launch` и запустите вместо него следующий файл `pub_tf_frame.launch`:

```text
$ roslaunch rbx2_utils pub_tf_frame.launch
```

Кадр, который мы хотим отслеживать, указывается в файле запуска параметром `target_frame`. По умолчанию этот кадр имеет значение '`right_gripper_link`'. Если узел заголовочного трекера еще не запущен, выведите его сейчас:

```text
$ roslaunch rbx2_dynamixels head_tracker.launch sim:=true
```

Возвращаясь в `RViz`, вы должны увидеть, как головная дорожка поворачивается вниз и вправо, чтобы посмотреть на правильный захват. Наконец, вызовите управление `arbotix_gui`, чтобы мы могли вручную перемещать руку.

```text
$ arbotix_gui
```

Используйте ползунок управления на arbotix\_gui для перемещения суставов руки. По мере перемещения ползунков, рука должна двигаться в RViz, а голова будет двигаться для отслеживания положения правого захвата. 

Теперь посмотрим на код.

```text
 #!/usr/bin/env python

 import rospy
 from geometry_msgs.msg import PoseStamped

 class PubFrame():
 def __init__(self):
 rospy.init_node('pub_tf_frame')

 # The rate at which we publish target messages
 rate = rospy.get_param('~rate', 20)

 # Convert the rate into a ROS rate
 r = rospy.Rate(rate)
 
 # The frame we want to track
 target_frame = rospy.get_param('~target_frame', 'right_gripper_link') 18
 # The target pose publisher
 target_pub = rospy.Publisher('target_pose', PoseStamped)

 # Define the target as a PoseStamped message
 target = PoseStamped()

 target.header.frame_id = target_frame

 target.pose.position.x = 0
 target.pose.position.y = 0
 target.pose.position.z = 0

 target.pose.orientation.x = 0
 target.pose.orientation.y = 0
 target.pose.orientation.z = 0
 target.pose.orientation.w = 1

 rospy.loginfo("Publishing target on frame " + str(target_frame)) 37
 while not rospy.is_shutdown():
 # Get the current timestamp
 target.header.stamp = rospy.Time.now()

 # Publish the target
 target_pub.publish(target) 44
 r.sleep()
 if __name__ ==
'__main__':
 try:
 target = PubFrame()
 rospy.spin()
 except rospy.ROSInterruptException:
 rospy.loginfo("Target publisher is shut down.")
```

Как видите, сценарий довольно простой. После чтения в параметре частоты публикаций мы получаем целевой кадр как параметр, который по умолчанию имеет значение `right_gripper_link` и может быть изменен в файле запуска. Затем мы определяем издателя для целевой темы `target_pose`. Затем мы устанавливаем целевой кадр с `PoseStamped` в качестве источника целевого кадра, присваивая ему координаты позиции \(0, 0, 0\) и значения ориентации \(0, 0, 0, 1\). Наконец, мы входим в цикл, чтобы опубликовать эту позу с новой меткой времени на каждом цикле. Вы можете задаться вопросом, публикуем ли мы каждый раз одну и ту же позицию, откуда головной трекер может знать, что связь на самом деле движется. Ответ заключается в том, что скрипт головного трекера использует функцию `tf transformPoint()` для поиска последнего преобразования между кадром камеры \(прикрепленным к голове\) и кадром `goal_pose` \(в данном случае прикрепленным к правой руке\). Таким образом, хотя координаты цели совпадают относительно кадра цели, сам кадр цели перемещается относительно остальной части робота. Поучительно попробовать разные координаты в скрипте `pub_tf_frame.py`. Например, попробуйте `target.pose.position.x = 0.1`, затем завершите и перезапустите файл запуска `pub_tf_frame.launch`, и теперь робот должен выглядеть в пределах 10 см от точки привязки захвата. Можно представить, что это может оказаться полезным, когда робот захватывает инструмент или другой объект, чтобы изображение с камеры можно было сфокусировать на объекте.



