# 9.3.2 Проецирование цели на плоскость камеры

Независимо от того, отслеживаем ли мы реальную или искусственную голову, нам нужно спроецировать 3D положение цели на плоскость камеры, чтобы мы знали, как далеко нужно панорамировать и наклонять камеру. Давайте воспользуемся функцией обратного вызова update\_joint\_speeds\(\) для изучения процесса. Обратный вызов update\_joint\_speeds\(\) выглядит следующим образом:

```text
def update_joint_speeds(self, msg):
# Acquire the lock
self.lock.acquire()

try:
# If message is empty, return immediately
if msg == PointStamped():
return

# If we get this far, the target
is visible
self.target_visible = True 12
# Get position component of the message
target = PointStamped()
target.header.frame_id = msg.header.frame_id
target.point = msg.pose.position
# Project the target point onto the camera link
camera_target = self.tf.transformPoint(self.camera_link, target)
# The virtual camera image is in the y-z plane
pan = -camera_target.point.y
tilt = -camera_target.point.z
# Compute the distance to the target in the x direction 26
distance = float(abs(camera_target.point.x))
# Convert the pan and tilt values from meters to radians 29
# Check for exceptions (NaNs) and use minimum range as fallback try:
pan /= distance 32
tilt /= distance except:
pan /= 0.5
tilt /= 0.5
# Get the current pan and tilt position 38
try:
current_pan =
self.joint_state.position[self.joint_state.name.index(self.head_pan_joint)]
current_tilt =
self.joint_state.position[self.joint_state.name.index(self.head_tilt_joint)]
except:
return
# Pan camera only if target displacement exceeds the threshold 45
if abs(pan) > self.pan_threshold:
# Set pan speed proportion to horizontal displacement
self.pan_speed = trunc(min(self.max_joint_speed, max(self.min_joint_speed,
self.gain_pan * abs(pan))), 48
if pan > 0:
self.pan_position = max(self.min_pan, current_pan -
self.lead_target_angle) else:
self.pan_position = min(self.max_pan, current_pan +
self.lead_target_angle) else:
self.pan_position = current_pan
self.pan_speed = self.min_joint_speed
# Tilt camera only if target displacement exceeds the threshold 
if abs(tilt) > self.tilt_threshold:
# Set tilt speed proportion to vertical displacement
self.tilt_speed = trunc(min(self.max_joint_speed, max(self.min_joint_speed,
self.gain_tilt * abs(tilt))), 61
if tilt < 0:
self.tilt_position = max(self.min_tilt, current_tilt -
self.lead_target_angle) else:
self.tilt_position = min(self.max_tilt, current_tilt +
self.lead_target_angle)
 else:
self.tilt_position = current_tilt
self.tilt_speed = self.min_joint_speed
 finally:
# Release the lock
self.lock.release()
```

Давайте посмотрим на ключевые строки функции обратного вызова.

```text
self.lock.acquire()
```

Сначала мы получили блокировку в начале обратного вызова. Это для защиты переменных `self.pan_speed, self.tilt_speed и self.target_visible`, которые также являются модифицированный в нашей главной петле.

```text
target = PointStamped()
target.header.frame_id = msg.header.frame_id
target.point = msg.pose.position
```

Перед проецированием сообщения `PoseStamped` на плоскость камеры мы извлекаем позиционный компонент, так как для этого процесса не требуется ориентировка цели.

```text
camera_target = self.tf.transformPoint(self.camera_link, target)
```

Здесь мы используем функцию transformPoint\(\) из библиотеки tf для проецирования мишени PointStamped на кадр камеры. \([Хорошую ссылку на другие функции преобразования можно найти в ROS Wiki на странице Использование Python tf](http://wiki.ros.org/tf/TfUsingPython)\).

```text
# The virtual camera image is in the y-z plane
pan = -camera_target.point.y
tilt = -camera_target.point.z

# Compute the distance to the target in the x direction
distance = float(abs(camera_target.point.x))
```

Помните, что кадр камеры ROS использует `y` для горизонтальной \(левая/правая\) оси и `z` для вертикальной \(вверх/вниз\) оси. Таким образом, мы изначально устанавливаем переменную `pan`, противоположную смещению мишени в направлении `y`, и аналогичным образом для переменной наклона в `z.`Ось x камеры направлена перпендикулярно плоскости камеры и, следовательно, представляет собой расстояние от цели до камеры.

```text
try:
 pan /= distance32 tilt /= distance 33 except:
 pan /= 0.5
 tilt /= 0.5
```

Нам нужно преобразовать значения панорамирования и наклона в угловые смещения \(в радиусах\), которые мы можем сделать, разделив каждую на расстояние до цели. В случае, если расстояние равно нулю, мы установите его на 0,5 метра, что примерно соответствует минимальному расстоянию, которое может выполнить Kinect или надёжно измерить Xtion Pro. 

Остальной скрипт по сути тот же самый, что и скрипт отслеживания головы, который мы использовали для  Первого тома и поэтому не будет описан далее здесь. 

В следующей главе мы воспользуемся этим новым узлом заголовочного трекера для поиска специального паттеры, называемые AR-тегами в трехмерном пространстве.

